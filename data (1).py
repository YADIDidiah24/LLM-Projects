# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fQYLiTATfncEHLNULwLtb5RxwcuaWXWw
"""

import re
import pandas as pd
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Optional, Tuple, Dict
import matplotlib.pyplot as plt
from tokenizers import Tokenizer, models, trainers, pre_tokenizers
import gc
import os

torch.backends.cudnn.benchmark = False  # Reduces memory usage
if torch.cuda.is_available():
    torch.cuda.empty_cache()

def read_whatsapp_chat(file_path: str) -> pd.DataFrame:
    # Define filtering patterns
    encryption_message = "Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more."
    media_pattern = "<Media omitted>"
    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}'
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    edited_message = "<This message was edited>"
    deleted_message = "You deleted this message"
    null_message = "null"
    created_group_message = "created group"
    added_you_to_group_message = "added you"
    tagging_pattern = r'@[\w]+'

    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # Apply filters to remove unwanted lines
    filtered_lines = []
    for line in lines:
        if (
            encryption_message not in line and
            deleted_message not in line and
            null_message != line.split(" ")[-1] and
            media_pattern not in line and
            created_group_message not in line and
            added_you_to_group_message not in line and
            not re.search(email_pattern, line) and
            not re.search(url_pattern, line)
        ):
            line = line.replace(edited_message, "").strip()
            line = re.sub(tagging_pattern, "", line).strip()
            filtered_lines.append(line)

    # Normalize content:
    content = '\n'.join(filtered_lines)
    # Replace narrow no-break space (iOS specific)
    content = content.replace('\u202f', ' ')
    # Remove square brackets if they surround the timestamp (only for iOS)
    content = re.sub(
        r'\[(\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?\s?[APap][Mm])\]',
        r'\1',
        content
    )
    # Remove LRM and RLM characters (Left-to-Right Mark and Right-to-Left Mark)
    content = content.replace('\u200E', '').replace('\u200F', '')

    # Updated regex pattern to match both iOS and Android WhatsApp exports.
    pattern = r'(\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?(?:\s?[APap][Mm])?)\s?(?:-|\~)?\s?(.*?): (.*?)(?=\n\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}|$)'
    messages = re.findall(pattern, content, re.DOTALL)
    df = pd.DataFrame(messages, columns=['timestamp', 'sender', 'message'])

    timestamps = []
    for timestamp in df['timestamp']:
        try:
            timestamp = pd.to_datetime(
                timestamp, format='mixed', errors='coerce')
        except Exception as e:
            print(f"Error parsing timestamp '{timestamp}': {e}")
            timestamp = pd.NaT
        timestamps.append(timestamp)

    df['timestamp'] = timestamps
    return df

all_chats = {}
file_name = "DummyData.txt"
all_chats[file_name] = read_whatsapp_chat(file_name)

text_sequence = ""
for file_name in all_chats.keys():
    text_sequence += " ".join(all_chats[file_name]['message'].values)

len(text_sequence)

with open("combined_text.txt", "w", encoding="utf-8") as f:
    f.write(text_sequence)

with open("combined_text.txt", "r", encoding="utf-8") as f:
    text_sequence = f.read()

len(text_sequence)

from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# Sample text
text = "Let's train a tiny tokenizer!"

# Create tokenizer
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# Train it
trainer = trainers.BpeTrainer(vocab_size=100, special_tokens=["<pad>", "<unk>"])
tokenizer.train_from_iterator([text], trainer=trainer)

# ✅ Add additional special tokens
tokenizer.add_special_tokens(["<|startoftext|>", "<|separator|>", "<|endoftext|>"])

# Confirm
vocab = tokenizer.get_vocab()
for token in ["<|startoftext|>", "<|separator|>", "<|endoftext|>"]:
    print(f"{token}: {vocab[token]}")

len(tokenizer.encode(text_sequence))

tokenizer.save("tokenizer.json")

from tokenizers import Tokenizer
tokenizer = Tokenizer.from_file("tokenizer.json")

tokenizer = Tokenizer(models.BPE())
tokenizer = Tokenizer.from_file("tokenizer.json")
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file("tokenizer.json")

def get_vocab_size(tokenizer: Tokenizer) -> int:
    return len(tokenizer.get_vocab())

print(get_vocab_size(tokenizer))

import torch
from typing import Optional, Tuple
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(3647)

block_size = 128
n_embd = 512
n_head = 4
n_layer = 4
dropout = 0.
vocab_size = get_vocab_size(tokenizer)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Enable memory efficient attention if available
if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
    use_flash_attention = True
else:
    use_flash_attention = False

class OptimizedHead(nn.Module):
    """Memory-efficient self-attention head"""

    def __init__(self, head_size: int):
        super().__init__()
        self.head_size = head_size
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)

        # Use buffer for causal mask
        self.register_buffer('causal_mask',
                           torch.tril(torch.ones(block_size, block_size)).bool())
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)

        if use_flash_attention and T > 32:  # Use flash attention for longer sequences
            # Use PyTorch's optimized attention
            out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=None,
                dropout_p=dropout if self.training else 0.0,
                is_causal=True
            )
        else:
            # Manual attention with gradient checkpointing
            wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)
            wei = wei.masked_fill(~self.causal_mask[:T, :T], float('-inf'))
            wei = F.softmax(wei, dim=-1)
            wei = self.dropout(wei)
            out = wei @ v

        return out

class OptimizedMultiHeadAttention(nn.Module):
    """Memory-efficient multi-head attention"""

    def __init__(self, num_heads: int, head_size: int):
        super().__init__()
        self.heads = nn.ModuleList([OptimizedHead(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(head_size * num_heads, n_embd, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class OptimizedFeedForward(nn.Module):
    """Smaller feedforward network"""

    def __init__(self, n_embd: int):
        super().__init__()
        # Reduced expansion factor from 4 to 2
        hidden_dim = 2 * n_embd
        self.net = nn.Sequential(
            nn.Linear(n_embd, hidden_dim, bias=False),
            nn.GELU(),  # More efficient than ReLU
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, n_embd, bias=False),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

class OptimizedBlock(nn.Module):
    """Memory-efficient transformer block with gradient checkpointing"""

    def __init__(self, n_embd: int, n_head: int):
        super().__init__()
        head_size = n_embd // n_head
        self.ln1 = nn.LayerNorm(n_embd, bias=False)
        self.attn = OptimizedMultiHeadAttention(n_head, head_size)
        self.ln2 = nn.LayerNorm(n_embd, bias=False)
        self.mlp = OptimizedFeedForward(n_embd)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Pre-norm architecture for better stability
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class OptimizedGPTLanguageModel(nn.Module):
    """Memory and compute optimized GPT model"""

    def __init__(self, vocab_size):  # Add vocab_size as parameter
        super().__init__()
        self.vocab_size = vocab_size
        self.tok_emb = nn.Embedding(vocab_size, n_embd)
        self.pos_emb = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[OptimizedBlock(n_embd, n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd, bias=False)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)

        # Weight tying to reduce parameters
        self.head.weight = self.tok_emb.weight

        # Initialize weights
        self.apply(self._init_weights)

        # Report parameter count
        print(f"Model parameters: {self.get_num_params()/1e6:.2f}M")

    def get_num_params(self, non_embedding=True):
        """Count the number of parameters in the model"""
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.pos_emb.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        # Check for out-of-range tokens
        if torch.max(idx) >= self.vocab_size:
            raise ValueError(f"Token ID {torch.max(idx).item()} is >= vocab_size {self.vocab_size}")

        # Token and position embeddings
        tok_emb = self.tok_emb(idx)
        pos_emb = self.pos_emb(torch.arange(T, device=device))
        x = tok_emb + pos_emb

        # Transformer blocks with gradient checkpointing for memory efficiency
        if self.training and T > 64:
            x = torch.utils.checkpoint.checkpoint_sequential(self.blocks, 2, x)
        else:
            x = self.blocks(x)

        x = self.ln_f(x)
        logits = self.head(x)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """Generate text with temperature and top-k sampling"""
        for _ in range(max_new_tokens):
            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / temperature

            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)

        return idx

model = OptimizedGPTLanguageModel(512)
model = model.to(device)
# print the number of parameters in the model
print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')

batch_size = 16
seq_length = 6
x = torch.randint(0, vocab_size, (batch_size, seq_length))
x = x.to(device)

logits, loss = model(x)
print(logits.shape, loss)

def print_model_structure(model: torch.nn.Module, indent: str = '') -> None:
    """
    Custom function to print model structure in a hierarchical format
    """
    for name, child in model.named_children():
        params = sum(p.numel() for p in child.parameters())
        print(f"{indent}├─ {name}: {child.__class__.__name__} ({params:,} parameters)")
        print_model_structure(child, indent + '│  ')

import pandas as pd


def get_model_stats(model: torch.nn.Module) -> pd.DataFrame:
    """
    Create a DataFrame with detailed layer statistics
    """
    stats = []
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # Only leaf modules
            params = sum(p.numel() for p in module.parameters())
            stats.append({
                'Layer Name': name,
                'Type': module.__class__.__name__,
                'Parameters': params,
                'Trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)
            })
    return pd.DataFrame(stats)


stats_df = get_model_stats(model)
stats_df

block_size = 256
n_embd = 512
n_head = 8
n_layer = 4
dropout = 0.2
batch_size = 64
vocab_size = get_vocab_size(tokenizer)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

model = OptimizedGPTLanguageModel(512).to(device)

print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')

with open("combined_text.txt", "r", encoding='utf-8') as f:
    text_sequence = f.read()

encoded_text_sequence = tokenizer.encode(text_sequence).ids
len(encoded_text_sequence)

data = torch.tensor(encoded_text_sequence, dtype=torch.long)
split_index = int(0.9*len(data))
train_data = data[:split_index]
val_data = data[split_index:]

# %%
class MemoryEfficientDataset(Dataset):
    """Memory-efficient dataset that loads data on demand"""

    def __init__(self, data: torch.Tensor, block_size: int):
        self.data = data
        self.block_size = block_size

    def __len__(self):
        return len(self.data) - self.block_size

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.block_size]
        y = self.data[idx + 1:idx + self.block_size + 1]
        return x, y

def create_small_tokenizer(text: str, vocab_size: int = 512):
    """Create a small, efficient tokenizer"""
    print(f"Creating tokenizer with vocab_size: {vocab_size}")
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["<pad>", "<unk>", "<|startoftext|>", "<|endoftext|>"],
        show_progress=True
    )

    tokenizer.train_from_iterator([text], trainer=trainer)

    # Verify the actual vocabulary size
    actual_vocab_size = len(tokenizer.get_vocab())
    print(f"Actual tokenizer vocab_size: {actual_vocab_size}")

    return tokenizer, actual_vocab_size

def validate_tokenization(tokenizer, text_sample):
    """Validate that tokenization works correctly"""
    # Test with a small sample
    sample = text_sample[:1000]
    encoded = tokenizer.encode(sample)
    max_token_id = max(encoded.ids) if encoded.ids else 0
    vocab_size = len(tokenizer.get_vocab())

    print(f"Sample encoding stats:")
    print(f"  Text length: {len(sample)}")
    print(f"  Token count: {len(encoded.ids)}")
    print(f"  Max token ID: {max_token_id}")
    print(f"  Vocab size: {vocab_size}")
    print(f"  Valid range: 0 to {vocab_size-1}")

    if max_token_id >= vocab_size:
        raise ValueError(f"Token ID {max_token_id} exceeds vocab_size {vocab_size}")

    return True

@torch.no_grad()
def estimate_loss(model, data_loader, eval_iters=50):
    """Estimate loss on a dataset"""
    model.eval()
    losses = []

    for i, (x, y) in enumerate(data_loader):
        if i >= eval_iters:
            break
        x, y = x.to(device), y.to(device)
        with torch.no_grad():
            _, loss = model(x, y)
        losses.append(loss.item())

    model.train()
    return sum(losses) / len(losses)

# Add these imports at the top of your file
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# %%
# Add these visualization functions after your model definition

def plot_training_curves(train_losses, val_losses, save_path='training_curves.png'):
    """Plot training and validation loss curves"""
    plt.figure(figsize=(12, 5))

    # Loss curves
    plt.subplot(1, 2, 1)
    epochs = range(len(train_losses))
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Learning rate over time (if you track it)
    plt.subplot(1, 2, 2)
    if len(train_losses) > 1:
        plt.plot(epochs, train_losses, 'g-', label='Training Loss Trend')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def analyze_tokenizer(tokenizer, text_sample, save_path='tokenizer_analysis.png'):
    """Analyze tokenizer statistics"""
    # Get vocabulary
    vocab = tokenizer.get_vocab()

    # Encode sample text
    encoded = tokenizer.encode(text_sample[:5000])  # Use first 5000 chars
    tokens = encoded.ids

    # Token frequency analysis
    token_counts = Counter(tokens)
    most_common_tokens = token_counts.most_common(20)

    # Decode most common tokens
    common_token_texts = []
    for token_id, count in most_common_tokens:
        try:
            token_text = tokenizer.decode([token_id])
            common_token_texts.append((token_text, count))
        except:
            common_token_texts.append((f"<{token_id}>", count))

    # Create visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Vocabulary size and token statistics
    axes[0, 0].bar(['Vocab Size', 'Unique Tokens\nin Sample', 'Total Tokens\nin Sample'],
                   [len(vocab), len(set(tokens)), len(tokens)],
                   color=['skyblue', 'lightcoral', 'lightgreen'])
    axes[0, 0].set_title('Tokenizer Statistics')
    axes[0, 0].set_ylabel('Count')

    # 2. Token frequency distribution
    token_ids, counts = zip(*most_common_tokens)
    axes[0, 1].bar(range(len(token_ids)), counts, color='orange')
    axes[0, 1].set_title('Top 20 Most Frequent Tokens')
    axes[0, 1].set_xlabel('Token Rank')
    axes[0, 1].set_ylabel('Frequency')

    # 3. Token length distribution
    token_lengths = [len(tokenizer.decode([token_id])) for token_id in set(tokens[:1000])]
    axes[1, 0].hist(token_lengths, bins=20, color='purple', alpha=0.7)
    axes[1, 0].set_title('Token Length Distribution')
    axes[1, 0].set_xlabel('Token Length (characters)')
    axes[1, 0].set_ylabel('Frequency')

    # 4. Compression ratio
    original_chars = len(text_sample[:5000])
    compressed_tokens = len(tokens)
    compression_ratio = original_chars / compressed_tokens

    axes[1, 1].bar(['Characters', 'Tokens'], [original_chars, compressed_tokens],
                   color=['red', 'blue'])
    axes[1, 1].set_title(f'Compression Ratio: {compression_ratio:.2f}:1')
    axes[1, 1].set_ylabel('Count')

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    # Print most common tokens
    print("\nMost Common Tokens:")
    for i, (token_text, count) in enumerate(common_token_texts[:10]):
        print(f"{i+1:2d}. '{token_text}' ({count} times)")

def plot_model_architecture(model, save_path='model_architecture.png'):
    """Visualize model architecture and parameters"""
    # Get layer information
    layer_info = []
    total_params = 0

    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # Only leaf modules
            params = sum(p.numel() for p in module.parameters())
            layer_info.append({
                'name': name,
                'type': module.__class__.__name__,
                'params': params
            })
            total_params += params

    # Create DataFrame
    df = pd.DataFrame(layer_info)
    df = df[df['params'] > 0]  # Only layers with parameters

    # Plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Parameters by layer type
    type_params = df.groupby('type')['params'].sum().sort_values(ascending=False)
    axes[0, 0].pie(type_params.values, labels=type_params.index, autopct='%1.1f%%')
    axes[0, 0].set_title('Parameters by Layer Type')

    # 2. Top layers by parameter count
    top_layers = df.nlargest(10, 'params')
    axes[0, 1].barh(range(len(top_layers)), top_layers['params'])
    axes[0, 1].set_yticks(range(len(top_layers)))
    axes[0, 1].set_yticklabels([name.split('.')[-1] for name in top_layers['name']], fontsize=8)
    axes[0, 1].set_title('Top 10 Layers by Parameter Count')
    axes[0, 1].set_xlabel('Parameters')

    # 3. Memory usage estimation
    param_memory = total_params * 4 / (1024**2)  # Assuming float32
    gradient_memory = param_memory  # Gradients same size as parameters
    activation_memory = 128 * 256 * 4 * 4 / (1024**2)  # Rough estimate

    memory_types = ['Parameters', 'Gradients', 'Activations']
    memory_sizes = [param_memory, gradient_memory, activation_memory]

    axes[1, 0].bar(memory_types, memory_sizes, color=['blue', 'orange', 'green'])
    axes[1, 0].set_title('Estimated Memory Usage (MB)')
    axes[1, 0].set_ylabel('Memory (MB)')

    # 4. Model summary
    axes[1, 1].axis('off')
    summary_text = f"""
    Model Summary:

    Total Parameters: {total_params:,}
    Model Size: {total_params * 4 / (1024**2):.1f} MB

    Architecture:
    - Embedding Dim: {model.tok_emb.embedding_dim}
    - Vocab Size: {model.vocab_size}
    - Layers: {len([m for m in model.modules() if isinstance(m, type(model.blocks[0]))])}
    - Attention Heads: {model.blocks[0].attn.heads.__len__() if hasattr(model.blocks[0], 'attn') else 'N/A'}
    """
    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def plot_generation_analysis(model, tokenizer, prompts=None, save_path='generation_analysis.png'):
    """Analyze text generation quality"""
    if prompts is None:
        prompts = ["Hello", "The weather is", "I think that", "In my opinion"]

    model.eval()

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    all_generations = []
    generation_lengths = []

    # Generate text for each prompt
    for prompt in prompts:
        input_ids = tokenizer.encode(prompt).ids
        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)

        with torch.no_grad():
            output = model.generate(
                input_tensor,
                max_new_tokens=50,
                temperature=0.8,
                top_k=40
            )

        generated_text = tokenizer.decode(output[0].tolist())
        all_generations.append(generated_text)
        generation_lengths.append(len(generated_text))

    # 1. Generation length distribution
    axes[0, 0].hist(generation_lengths, bins=10, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Generated Text Lengths')
    axes[0, 0].set_xlabel('Character Count')
    axes[0, 0].set_ylabel('Frequency')

    # 2. Token diversity in generations
    all_tokens = []
    for gen in all_generations:
        tokens = tokenizer.encode(gen).ids
        all_tokens.extend(tokens)

    unique_tokens = len(set(all_tokens))
    total_tokens = len(all_tokens)
    diversity_ratio = unique_tokens / total_tokens if total_tokens > 0 else 0

    axes[0, 1].bar(['Unique Tokens', 'Total Tokens'], [unique_tokens, total_tokens],
                   color=['orange', 'blue'])
    axes[0, 1].set_title(f'Token Diversity (Ratio: {diversity_ratio:.3f})')
    axes[0, 1].set_ylabel('Count')

    # 3. Most common generated tokens
    token_counts = Counter(all_tokens)
    common_tokens = token_counts.most_common(10)

    if common_tokens:
        token_ids, counts = zip(*common_tokens)
        axes[1, 0].bar(range(len(token_ids)), counts, color='green')
        axes[1, 0].set_title('Most Common Generated Tokens')
        axes[1, 0].set_xlabel('Token Rank')
        axes[1, 0].set_ylabel('Frequency')

    # 4. Sample generations
    axes[1, 1].axis('off')
    sample_text = "Sample Generations:\n\n"
    for i, gen in enumerate(all_generations[:3]):
        sample_text += f"{i+1}. {gen[:100]}...\n\n"

    axes[1, 1].text(0.05, 0.95, sample_text, fontsize=10, verticalalignment='top',
                    wrap=True, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def plot_loss_distribution(train_losses, val_losses, save_path='loss_distribution.png'):
    """Plot loss distribution and statistics"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # 1. Loss distribution
    axes[0].hist(train_losses, bins=20, alpha=0.7, label='Train', color='blue')
    axes[0].hist(val_losses, bins=20, alpha=0.7, label='Validation', color='red')
    axes[0].set_xlabel('Loss Value')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Loss Distribution')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 2. Loss improvement over time
    if len(train_losses) > 1:
        train_improvement = [train_losses[0] - loss for loss in train_losses]
        val_improvement = [val_losses[0] - loss for loss in val_losses]

        epochs = range(len(train_losses))
        axes[1].plot(epochs, train_improvement, 'b-', label='Train Improvement')
        axes[1].plot(epochs, val_improvement, 'r-', label='Val Improvement')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss Improvement')
        axes[1].set_title('Loss Improvement Over Time')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

    # 3. Statistics
    axes[2].axis('off')
    stats_text = f"""
    Training Statistics:

    Train Loss:
    - Final: {train_losses[-1]:.4f}
    - Best: {min(train_losses):.4f}
    - Mean: {np.mean(train_losses):.4f}
    - Std: {np.std(train_losses):.4f}

    Validation Loss:
    - Final: {val_losses[-1]:.4f}
    - Best: {min(val_losses):.4f}
    - Mean: {np.mean(val_losses):.4f}
    - Std: {np.std(val_losses):.4f}

    Overfitting Check:
    - Gap: {val_losses[-1] - train_losses[-1]:.4f}
    """

    axes[2].text(0.05, 0.95, stats_text, fontsize=11, verticalalignment='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcyan"))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

# %%
# Modified train_model function to include tracking and visualizations
def train_model():
    """Optimized training loop with comprehensive visualizations"""

    # Load and prepare data
    print("Loading data...")
    all_chats = {}
    file_name = "DummyData.txt"
    all_chats[file_name] = read_whatsapp_chat(file_name)
    learning_rate = 3e-4
    # Combine text
    text_sequence = " ".join(all_chats[file_name]['message'].values)
    print(f"Text length: {len(text_sequence)} characters")

    # Create tokenizer with specified vocabulary size
    target_vocab_size = 512
    print("Creating tokenizer...")
    tokenizer, actual_vocab_size = create_small_tokenizer(text_sequence, target_vocab_size)

    # Analyze tokenizer before training
    print("Analyzing tokenizer...")
    analyze_tokenizer(tokenizer, text_sequence)

    # Validate tokenization
    validate_tokenization(tokenizer, text_sequence)

    # Encode text
    print("Encoding text...")
    encoded_text = tokenizer.encode(text_sequence).ids
    data = torch.tensor(encoded_text, dtype=torch.long)
    print(f"Encoded length: {len(data)} tokens")

    # Double-check for out-of-range tokens
    max_token = torch.max(data).item()
    min_token = torch.min(data).item()
    print(f"Token range in data: {min_token} to {max_token}")
    print(f"Expected range: 0 to {actual_vocab_size-1}")

    if max_token >= actual_vocab_size:
        raise ValueError(f"Data contains token {max_token} which is >= vocab_size {actual_vocab_size}")

    # Train/val split
    split_idx = int(0.9 * len(data))
    train_data = data[:split_idx]
    val_data = data[split_idx:]

    # Create datasets
    train_dataset = MemoryEfficientDataset(train_data, block_size)
    val_dataset = MemoryEfficientDataset(val_data, block_size)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    # Create model with correct vocab_size
    print(f"Creating model with vocab_size: {actual_vocab_size}...")
    model = OptimizedGPTLanguageModel(actual_vocab_size).to(device)

    # Visualize model architecture
    plot_model_architecture(model)

    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=0.1,
        eps=1e-8
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=1000, eta_min=learning_rate/10
    )

    # Training loop with tracking
    max_iters = 5
    eval_interval = 50
    save_interval = 200

    train_losses = []
    val_losses = []
    batch_losses = []  # Track individual batch losses
    learning_rates = []  # Track learning rate changes

    print("Starting training...")
    try:
        for iter_num in range(max_iters):
            model.train()
            epoch_losses = []

            for batch_idx, (x, y) in enumerate(train_loader):
                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)

                # Validate batch tokens
                if torch.max(x) >= actual_vocab_size or torch.max(y) >= actual_vocab_size:
                    print(f"Batch contains out-of-range tokens!")
                    print(f"x range: {torch.min(x)} to {torch.max(x)}")
                    print(f"y range: {torch.min(y)} to {torch.max(y)}")
                    raise ValueError("Out-of-range tokens in batch")

                # Forward pass
                logits, loss = model(x, y)
                epoch_losses.append(loss.item())
                batch_losses.append(loss.item())
                learning_rates.append(optimizer.param_groups[0]['lr'])

                # Backward pass
                optimizer.zero_grad(set_to_none=True)
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

                optimizer.step()
                scheduler.step()

                # Free memory
                del logits, loss
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # Print progress
                if batch_idx % 10 == 0:
                    avg_loss = sum(epoch_losses) / len(epoch_losses)
                    print(f"Iter {iter_num}, Batch {batch_idx}: loss {avg_loss:.4f}, lr {learning_rates[-1]:.6f}")

                # Break early for testing
                if batch_idx >= 20:
                    break

            # Evaluation
            if iter_num % 2 == 0:
                train_loss = estimate_loss(model, train_loader, eval_iters=5)
                val_loss = estimate_loss(model, val_loader, eval_iters=5)

                train_losses.append(train_loss)
                val_losses.append(val_loss)

                print(f"Epoch {iter_num}: train loss {train_loss:.4f}, val loss {val_loss:.4f}")

                # Plot intermediate results
                if len(train_losses) > 1:
                    plot_training_curves(train_losses, val_losses)

    except Exception as e:
        print(f"Training error: {e}")
        torch.save({
            'model': model.state_dict(),
            'vocab_size': actual_vocab_size,
            'error': str(e)
        }, 'error_checkpoint.pth')
        raise

    # Final visualizations
    print("Creating final visualizations...")

    # 1. Training curves
    plot_training_curves(train_losses, val_losses)

    # 2. Loss distribution analysis
    if len(train_losses) > 0 and len(val_losses) > 0:
        plot_loss_distribution(train_losses, val_losses)

    # 3. Generation analysis
    plot_generation_analysis(model, tokenizer)

    # 4. Batch loss progression
    if len(batch_losses) > 10:
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.plot(batch_losses, alpha=0.7)
        plt.title('Batch Loss Progression')
        plt.xlabel('Batch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 2, 2)
        plt.plot(learning_rates, color='orange')
        plt.title('Learning Rate Schedule')
        plt.xlabel('Batch')
        plt.ylabel('Learning Rate')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('batch_progression.png', dpi=300, bbox_inches='tight')
        plt.show()

    # Save final model
    torch.save({
        'model': model.state_dict(),
        'vocab_size': actual_vocab_size,
        'config': {
            'block_size': block_size,
            'n_embd': n_embd,
            'n_head': n_head,
            'n_layer': n_layer
        },
        'training_history': {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'batch_losses': batch_losses,
            'learning_rates': learning_rates
        }
    }, 'final_model.pth')

    tokenizer.save("tokenizer.json")

    print("Training completed successfully!")
    print("All visualizations saved as PNG files.")
    return model, tokenizer

# %%
# Enhanced test_generation function with visualization
def test_generation_with_viz(model, tokenizer, prompts=None, max_tokens=50):
    """Test text generation with visualization"""
    if prompts is None:
        prompts = ["Hello world", "The weather today", "I believe that", "In the future"]

    model.eval()
    results = []

    print("Generating text samples...")
    print("="*50)

    for i, prompt in enumerate(prompts):
        input_ids = tokenizer.encode(prompt).ids
        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)

        with torch.no_grad():
            output = model.generate(
                input_tensor,
                max_new_tokens=max_tokens,
                temperature=0.8,
                top_k=40
            )

        generated_text = tokenizer.decode(output[0].tolist())
        results.append({
            'prompt': prompt,
            'generated': generated_text,
            'length': len(generated_text)
        })

        print(f"{i+1}. Prompt: '{prompt}'")
        print(f"   Generated: '{generated_text}'")
        print(f"   Length: {len(generated_text)} characters")
        print("-" * 50)

    # Create results DataFrame
    df_results = pd.DataFrame(results)

    # Visualize results
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.bar(range(len(results)), [r['length'] for r in results])
    plt.title('Generated Text Lengths')
    plt.xlabel('Sample')
    plt.ylabel('Characters')
    plt.xticks(range(len(results)), [f"Sample {i+1}" for i in range(len(results))])

    plt.subplot(2, 2, 2)
    # Token diversity analysis
    all_generated_tokens = []
    for result in results:
        tokens = tokenizer.encode(result['generated']).ids
        all_generated_tokens.extend(tokens)

    from collections import Counter
    token_freq = Counter(all_generated_tokens)
    top_tokens = token_freq.most_common(10)

    if top_tokens:
        token_ids, frequencies = zip(*top_tokens)
        plt.bar(range(len(token_ids)), frequencies)
        plt.title('Most Common Generated Tokens')
        plt.xlabel('Token Rank')
        plt.ylabel('Frequency')

    plt.subplot(2, 1, 2)
    plt.axis('off')

    # Create a text summary
    summary_text = "Generation Summary:\n\n"
    for i, result in enumerate(results[:3]):  # Show first 3
        summary_text += f"{i+1}. '{result['prompt']}' → '{result['generated'][:80]}...'\n\n"

    plt.text(0.05, 0.95, summary_text, fontsize=10, verticalalignment='top',
            transform=plt.gca().transAxes, wrap=True,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))

    plt.tight_layout()
    plt.savefig('generation_test_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return df_results

def test_generation(model, tokenizer, prompt="Hello", max_tokens=50):
    """Test text generation"""
    model.eval()

    input_ids = tokenizer.encode(prompt).ids
    input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)

    with torch.no_grad():
        output = model.generate(
            input_tensor,
            max_new_tokens=max_tokens,
            temperature=0.8,
            top_k=40
        )

    generated_text = tokenizer.decode(output[0].tolist())
    print(f"Generated: {generated_text}")

def optimize_for_low_memory():
    """Apply memory optimization settings"""

    # PyTorch settings for low memory
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    if torch.cuda.is_available():
        # Set memory fraction to leave room for system
        torch.cuda.set_per_process_memory_fraction(0.8)
        torch.cuda.empty_cache()

    # Reduce number of threads for CPU
    torch.set_num_threads(2)

    print("Optimizations applied for low-end hardware")

# Apply optimizations
optimize_for_low_memory()

# Run training (uncomment to execute)
model, tokenizer = train_model()
test_generation(model, tokenizer, "Hello world", 30)

import re
import pandas as pd
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from tokenizers import Tokenizer, models, trainers, pre_tokenizers
import os
from typing import Optional, Tuple

# Configuration
class Config:
    # Model parameters
    vocab_size = 2048  # Increased for better coverage
    block_size = 256
    n_embd = 384
    n_head = 6
    n_layer = 6
    dropout = 0.1

    # Training parameters
    batch_size = 32
    learning_rate = 3e-4
    max_epochs = 20
    eval_interval = 5
    warmup_steps = 100

    # System
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

def read_whatsapp_chat(file_path: str) -> pd.DataFrame:
    """Parse WhatsApp chat export file."""

    # Define patterns for filtering
    filters = [
        "Messages and calls are end-to-end encrypted",
        "<Media omitted>",
        "<This message was edited>",
        "You deleted this message",
        "created group",
        "added you"
    ]

    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}'
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    tagging_pattern = r'@[\w]+'

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Clean content
    content = content.replace('\u202f', ' ')  # Replace narrow no-break space
    content = content.replace('\u200E', '').replace('\u200F', '')  # Remove directional marks

    # Remove iOS square brackets around timestamps
    content = re.sub(
        r'\[(\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?\s?[APap][Mm])\]',
        r'\1', content
    )

    # Extract messages with regex
    pattern = r'(\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?(?:\s?[APap][Mm])?)\s?(?:-|\~)?\s?(.*?): (.*?)(?=\n\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}|$)'
    messages = re.findall(pattern, content, re.DOTALL)

    if not messages:
        raise ValueError("No messages found. Check the file format.")

    df = pd.DataFrame(messages, columns=['timestamp', 'sender', 'message'])

    # Filter out unwanted messages
    mask = pd.Series([True] * len(df))
    for filter_text in filters:
        mask &= ~df['message'].str.contains(filter_text, case=False, na=False)

    mask &= ~df['message'].str.contains(email_pattern, case=False, na=False)
    mask &= ~df['message'].str.contains(url_pattern, case=False, na=False)

    df = df[mask].copy()

    # Clean messages
    df['message'] = df['message'].str.replace(tagging_pattern, '', regex=True).str.strip()
    df = df[df['message'].str.len() > 0]  # Remove empty messages

    return df

def create_tokenizer(text: str, vocab_size: int) -> Tokenizer:
    """Create and train a BPE tokenizer."""

    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["<pad>", "<unk>", "<|startoftext|>", "<|endoftext|>"],
        show_progress=False
    )

    tokenizer.train_from_iterator([text], trainer=trainer)

    return tokenizer

class TextDataset(Dataset):
    """Efficient dataset for language modeling."""

    def __init__(self, data: torch.Tensor, block_size: int):
        self.data = data
        self.block_size = block_size

    def __len__(self):
        return len(self.data) - self.block_size

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.block_size]
        y = self.data[idx + 1:idx + self.block_size + 1]
        return x, y

class MultiHeadAttention(nn.Module):
    """Multi-head self-attention with optional flash attention."""

    def __init__(self, n_embd: int, n_head: int, dropout: float):
        super().__init__()
        assert n_embd % n_head == 0

        self.n_head = n_head
        self.n_embd = n_embd
        self.head_size = n_embd // n_head

        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)
        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.dropout = dropout

        # Causal mask
        self.register_buffer('causal_mask', torch.tril(torch.ones(Config.block_size, Config.block_size)))

    def forward(self, x):
        B, T, C = x.shape

        # Calculate query, key, values for all heads in batch
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)

        # Reshape for multi-head attention
        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)
        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)

        # Use flash attention if available
        if hasattr(F, 'scaled_dot_product_attention'):
            out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=True
            )
        else:
            # Manual attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / (self.head_size ** 0.5))
            att = att.masked_fill(self.causal_mask[:T, :T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            att = F.dropout(att, p=self.dropout, training=self.training)
            out = att @ v

        # Reassemble all head outputs side by side
        out = out.transpose(1, 2).contiguous().view(B, T, C)

        # Output projection
        out = self.c_proj(out)
        return out

class MLP(nn.Module):
    """Feed-forward network."""

    def __init__(self, n_embd: int, dropout: float):
        super().__init__()
        self.c_fc = nn.Linear(n_embd, 4 * n_embd, bias=False)
        self.gelu = nn.GELU()
        self.c_proj = nn.Linear(4 * n_embd, n_embd, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):
    """Transformer block."""

    def __init__(self, n_embd: int, n_head: int, dropout: float):
        super().__init__()
        self.ln_1 = nn.LayerNorm(n_embd)
        self.attn = MultiHeadAttention(n_embd, n_head, dropout)
        self.ln_2 = nn.LayerNorm(n_embd)
        self.mlp = MLP(n_embd, dropout)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

class GPTModel(nn.Module):
    """GPT Language Model."""

    def __init__(self, config: Config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config.n_embd, config.n_head, config.dropout)
                              for _ in range(config.n_layer)]),
            ln_f = nn.LayerNorm(config.n_embd),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # Weight tying
        self.transformer.wte.weight = self.lm_head.weight

        # Initialize weights
        self.apply(self._init_weights)

        print(f"Model initialized with {self.get_num_params()/1e6:.1f}M parameters")

    def get_num_params(self, non_embedding=True):
        """Count the number of parameters."""
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()

        assert t <= self.config.block_size, f"Sequence length {t} exceeds block size {self.config.block_size}"

        # Token embeddings and position embeddings
        pos = torch.arange(0, t, dtype=torch.long, device=device)
        tok_emb = self.transformer.wte(idx)
        pos_emb = self.transformer.wpe(pos)
        x = self.transformer.drop(tok_emb + pos_emb)

        # Transformer blocks
        for block in self.transformer.h:
            x = block(x)

        # Final layer norm and projection to vocab
        x = self.transformer.ln_f(x)

        if targets is not None:
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            logits = self.lm_head(x[:, [-1], :])  # Only compute for last token
            loss = None

        return logits, loss

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """Generate text autoregressively."""
        for _ in range(max_new_tokens):
            # Crop sequence if too long
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]

            # Forward pass
            logits, _ = self(idx_cond)

            # Scale by temperature and apply top-k filtering
            logits = logits[:, -1, :] / temperature

            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # Sample from the distribution
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)

            # Append to sequence
            idx = torch.cat((idx, idx_next), dim=1)

        return idx

@torch.no_grad()
def estimate_loss(model, data_loader, eval_iters=100):
    """Estimate loss on a dataset."""
    model.eval()
    losses = torch.zeros(eval_iters)

    for k, (x, y) in enumerate(data_loader):
        if k >= eval_iters:
            break
        x, y = x.to(Config.device), y.to(Config.device)
        with torch.no_grad():
            _, loss = model(x, y)
        losses[k] = loss.item()

    out = losses.mean()
    model.train()
    return out

def train_model(config: Config, train_data: str, model_save_path: str = "gpt_model.pt"):
    """Train the GPT model."""

    # Create tokenizer
    print("Creating tokenizer...")
    tokenizer = create_tokenizer(train_data, config.vocab_size)
    actual_vocab_size = len(tokenizer.get_vocab())
    config.vocab_size = actual_vocab_size

    # Encode text
    print("Encoding text...")
    encoded = tokenizer.encode(train_data).ids
    data = torch.tensor(encoded, dtype=torch.long)

    # Train/validation split
    n = len(data)
    train_data = data[:int(n*0.9)]
    val_data = data[int(n*0.9):]

    # Create datasets
    train_dataset = TextDataset(train_data, config.block_size)
    val_dataset = TextDataset(val_data, config.block_size)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    # Initialize model
    model = GPTModel(config).to(config.device)

    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.1)

    # Learning rate scheduler
    def get_lr(it):
        import math
        # Linear warmup
        if it < config.warmup_steps:
            return config.learning_rate * it / config.warmup_steps
        # Cosine decay
        return config.learning_rate * 0.5 * (1.0 + math.cos(math.pi * it / (config.max_epochs * len(train_loader))))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)

    print("Starting training...")
    best_val_loss = float('inf')

    for epoch in range(config.max_epochs):
        model.train()
        total_loss = 0

        for batch_idx, (x, y) in enumerate(train_loader):
            x, y = x.to(config.device), y.to(config.device)

            # Forward pass
            _, loss = model(x, y)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)

        # Evaluation
        if epoch % config.eval_interval == 0:
            val_loss = estimate_loss(model, val_loader, eval_iters=50)
            print(f"Epoch {epoch}: train_loss={avg_loss:.4f}, val_loss={val_loss:.4f}")

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'model': model.state_dict(),
                    'config': config,
                    'vocab_size': actual_vocab_size,
                    'best_val_loss': best_val_loss
                }, model_save_path)
                print(f"Saved best model (val_loss={val_loss:.4f})")

    # Save tokenizer
    tokenizer.save("tokenizer.json")
    print(f"Training completed. Best validation loss: {best_val_loss:.4f}")

    return model, tokenizer

def generate_text(model, tokenizer, prompt: str, max_tokens: int = 100):
    """Generate text from a prompt."""
    model.eval()

    # Encode prompt
    tokens = tokenizer.encode(prompt).ids
    x = torch.tensor(tokens, dtype=torch.long, device=Config.device)[None, ...]

    # Generate
    with torch.no_grad():
        y = model.generate(x, max_tokens, temperature=0.8, top_k=40)

    # Decode
    generated = tokenizer.decode(y[0].tolist())
    return generated

# Example usage
def main():
    # Read WhatsApp data
    df = read_whatsapp_chat("DummyData.txt")
    text_data = " ".join(df['message'].values)

    print(f"Loaded {len(text_data)} characters of text")

    # Initialize config
    config = Config()

    # Train model
    model, tokenizer = train_model(config, text_data)

    # Test generation
    prompts = ["Hello", "The weather is", "I think that"]
    for prompt in prompts:
        generated = generate_text(model, tokenizer, prompt, max_tokens=50)
        print(f"Prompt: '{prompt}' -> Generated: '{generated}'")

if __name__ == "__main__":
    main()